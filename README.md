# industrial-resources-and-its-impact

## Folder organization  

<pre>
.
├──data 
    ├── collecting
    ├── processed
    ├── processing
        ├── DC01
        ├── DC02
        ├── PD01 
        ├── PD02 
        ├── PD03 
        ├── PD04
        └── PD05
    └── raw 
        ├── macro_daily
        ├── ppi_and_cpi
        └── stock
├── EDA 
    └── Yuan 
├── Erdos_milestone
    ├── Mar7_2025
    └── Mar21_2025
├── model_training
    ├── EBM
    ├── NN
    ├── spline
    └── SPLINE_YUAN
└── proj_mod
</pre>

## Folder explanation
./data/collecting contains one file that collets PPI data with API. 

./data/processed contains all processed files generated by ipynb files.  

./data/processing organizes by folders steps of data processing.  

./data/raw organizes by categories the raw data files.  

./EDA organizes by partcipant the EDA related notebooks.  

./Erdos_milestone organizes by checkpoints the Erdos milestone notebooks.  

./model_training organizes by models the notebooks for model trainings.  

./proj_mod is the self created python module containing functions used accross files.  

## File naming explanation 
All ipynb files are named with format: 

[*task code*]_[*further explanations*].ipynb 

See following the *task code* and *task* correspondence: 

| task code | task explanation | 
|-----------|-------------------|
|DC01|Identify stable tech company (i.e. a tech compnay, classified with NAICS, maintaining sp 500 index status during a continuous period that we care)|
|DC02|Identify the NAICS codes that we consider tech industry| 
|DCO02|Collecting PPI data according to the NAICS chosen with API| 
|PD01|Created the *tech (stock) index*|
|PD02|Create proportional change of tech index| 
|PD03|Processing daily macro data, this includes fed rates, resource prices, and alikes|
|PD04|Processing monthly macro data, this includes PPI data| 
|EDA|Doing EDA and playing around with the data| 
|PD05|Create main datasets, this includes creating the no inflation adjusted dataset and creating the inlfation adjusted dataset| 
|MTEBM|Training related files for Explanable Boosting Machine| 
|MTMARS|Training related files for Multivariate Adaptive Regression Splines| 
|MTNN|Training related files for Neural Networks| 
|MTSPLINE|Training related files for Spline model| 

## Contributors 
Kayode Oyedele, Rafael Almeida Fernandes, Thiago Brasileiro Feitosa, Wojciech Tralle, Xiangyi Tao, and Yuan Zhang. 

## Motivation 
It is a macroeconomics fact that the cost of production has an impact on the performance of an industry. We expect that the stock data of relevant industries will be a valid indicator of the performance of the industry. We, hence, look to use macroeconomic data and price data on industrial resources as parameters to train a supervised model on the target of stock data. In particular, we will focus solely on the technological industry in this project. We will care about the impact of contextual variables on the tech stock index: so instead of looking at the stock index itself, we will focus on the proportional change of the stock index relative to certain numbers of previous trading days; and, similarly, instead of the value of the parameters, we focus on the proportional change of those variables. However, certain variables are contextual on their own, for instance, the federal funds rate. 

As a clarification: the goal is NOT to predict the future of the stock market (e.g. predicting the index of tomorrow with data we have today). Instead, we are using the stock index as a representation of the state of the industry, and attempt to model the change in that state based on change on macroeconomic parameters (e.g. we may use proportional price change of copper today as one of the parameters for the target of proportional change of stock index of today). 

## Raw data collection 

* Daily data on the total S and P 500 stock index between 2009 and 2025. 
* Daily data of relative weight contribution of each S and P 500 companies. Notice that: Companies enter and exit S and P 500 throughout time depending on their performance. 
* SIC (Standard Industrial Classification codes), and stock tickers of companies.  
* The SIC to NAICS (North American Industry Classification System) crosswalk data. 
* Monthly data of PPI (Producer Price Index) classified by NAICS. These data are used as part of the parameters indicating the cost of production. 
* Daily data on raw industrial resources involved in the tech industry: gold, silver, copper, crude oil, palladium, platinum. These data are used as parameters influencing production costs. 
* Daily data on federal fund rates. These data are used as parameters. 
* Monthly data of CPI (Consumer Price Index). These data are used as indicators of general inflation. 

## Data Processing 

* Classifying the companies:
  
    - Challenge: Since the PPI data are classified by NAICS, but no raw data matching companies directly to NAICS exists.
  
      + Solution: combining raw data assigning companies to SIC codes and the crosswalk data on SIC to NAICS to produce a map from companies to NAICS codes.
      
    - Result: Produced, in accordance with NAICS, a list of companies considered as “tech industry”, and retrieved (monthly) PPI data according to the NAICS chosen.

* Producing Weighted Stock index of tech companies (i.e. the *tech (stock) index*):
  
    - Challenge: Companies leave the S and P 500 index periodically depending on their performance, and sometimes they also change names and/or stock tickers. To prevent “Jumps” happening in the tech stock index, we need to identify the “stable companies” that did not leave or enter the S and P 500 index in a period that we are interested in.
      
        + Solution: Created a dictionary mapping each company to all of its stock tickers used in history (as identifiers). Then identified the “stable companies” between 2014 and 2024 by ensuring there are no missing values on each trading date between the start and end.
          
    - Result: Created a list of “stable companies”, and combining the raw data on their weight contribution and total S and P index to receive the daily data on tech stock index:
```math
\text{Tech stock index}:= \text{Total index}\times(\sum_{\begin{matrix}\text{company }\in \\ \text{Stable tech companies}\end{matrix}}{\text{weight contribution of the company}})
```

* Creating proportional change daily data:
  
    - Challenge: sporadically missing data on certain days for price of raw resources.
      
        + Solution: Patched them up “linearly”.
          
    - Result: Generated daily proportional change data (recorded as percentage data by multiplying with 100) for each of the tech stock index and the raw resources with formula:
```math
\begin{matrix}
    \text{proportional change on day }t \\ 
    \text{relative to the }n^{th}\text{ prior day}
\end{matrix}:=\frac{\text{value on day }t}{\text{value on day }t-n}-1
```

* Create proportional change monthly data:
  
    - Result: Calculated the proportional change data in the context of monthly values and “expanded” them to daily values in the following way (e.g.): 
```math
\text{“Proportional change” on Jan/06/2018}:=\text{Proportional change on Jan/2018}
```

**Remark:** Due to the raw data collected, there is a chunk of missing data on tech stock index beginning Dec/31/2016, ending Jul/05/2017. This chunk is "too big to patch up" responsibly 

**Remark:** In EDA, it is noticed that the parameters are not highly correlated to the target (proportional change of tech stock index). And it appears that the proportional change of tech stock index tends to increase with the increase of proportional change of price of raw resources (this is opposed to our intuition). We **conjecture** that the counter intuitive trend can be explained by inflation, and thus the following step. 

* Create inflation adjusted data:
  
    - Challenge: Inflation is conventionally defined as the change rate of CPI (Customer Price index). This data is only in monthly format, but our data are all daily.
      
        + Solution: Instead of using proportional changes relative to 1st (trading) day prior, we will use the proportional changes relative to 20th (trading) day prior. As a remark, there are about 20 trading days in a calendar month. 

    - Result: Created inflation adjusted data relative to 20th (trading) day prior with formula:  
```math
\begin{matrix}
    \text{Inflation adjusted} \\ 
    \text{proportional change on day } t
\end{matrix}:=\frac{\text{value on day }t}{\text{value on day }t-20}\times \frac{\text{CPI precious month}}{\text{CPI current month}}-1
```

**Remark:** After another round of EDA, it can be seen that some of the correlations increased, but the counterintuitive behavior is intensified, this disproves our conjecture. 

## Datasets Created 

* No inflation adjusted dataset: For each row (one for each trading day), we have in columns:

    - **Target**: proportional change of tech stock index relative to previous trading day.
      
    - **Parameters**: proportional change of price of raw resources relative to previous trading day, proportional change of PPI’s relative to previous month, federal fund rate.
      
    - **Train and test split**: Test set start on date 2024-Jan-01 (Target has (Population) Variance: 1.6098). 

* Inflation adjusted dataset: For each row (one for each trading day), we have in columns: 

    - **Target**: proportional change of tech stock index relative to 20th trading day prior. 

    - **Parameters**: proportional change of price of raw resources relative to 20th trading day prior, proportional change of PPI’s relative to previous month, federal fund rate.
 
    - **Train and test split**: Test set start on date 2024-Jan-01 (Target has (Population) Variance: 24.1491). 

## Models, cross-validations, and results 

*The metric we will be using is mse (mean squared error). *

* **EBM (Explainable Boosting Machine)**: 

    - EBM team: Yuan, Kayode, Wojciech. 

    - Pipeline: 1,Index shifting to include more historical data -> 2, Exclude or include extreme training data -> 3, Standard scalar -> 4, EBM
 
    - Cross-validation: We decided to do cross-validation on changing shifting numbers (which decide how much historical data to include, the higher the shifting number, the more history is included), and if we were to include extreme training data. The cross-validation is done with time series 5-fold. 

    - Results:

       + **On the not inflation adjusted data**: It turns out that the higher the shifting, the better the EBM pipeline performs on validation sets without outliers, but worse when including outliers in the validation set. The impact of including or excluding outliers in the training data is small when the shifting number is small, but training with outliers produces better results when the shifting number is high. Our preferred final models produce: mse 1.5846 (R^2 value 0.0157) when trained with outliers and 10 shiftings, and mse 1.5694 (R^2 value 0.0251) when trained with outliers without any shiftings.
     
       + **On the inflation adjusted data**: It appears that training without outliers is preferred when the shifting number is low, but the difference is small when the shifting number is high. The error increases at first but then reduces while the shifting number increases. Including outliers in the validation set or not did not make much of a difference to the trends. Our preferred final model produces: mse 38.4721 (R^2 value -0.5931) when trained with extreme data and 3 shiftings. 

* **NN (Neural Network)**: 

    - NN team: Yuan, Kayode, Wojciech. 

    - Clarification: We only worked on the dataset with inflation adjustment. 

    - Cross-validation: We created two different models, one with a RNN (recurrent neural network) to include the impact of historical data, and a MLP with attention layer to include in the consideration of interactions. For each of these, we proceed with cross-validation by changing epoch numbers of each model. The attention layer model also includes cross-validation through including different lags to include lagged data from history. The RNN also lag length (determining how much past data to include). 

    - Results:

      + **RNN**: Our preferred model produced mse 31.4635 (R^2 value -0.3029) with 190 epochs and 6 lag length.
     
      + **MLP with Attention layer model**: Our preferred model produced mse 23.06 (R^2 value 0.0451) with no lag and 300 epochs.
     
* **SPLINE**: 

    - SPLINE team: Xiangyi, Thiago, Rafael.
 
    - Pipeline: 1, Exclude or include extreme training data -> 2, Standard scalar -> 3, SPLINE

    - Cross-validation: We used a five-fold time-series cross-validation. We explored different values for the spline transformer’s parameters (trying out several n_knots and degree settings by manually adjusting them). Through these manual checks, we found that n_knots=2 and degree=1 consistently yielded the lowest mse in our cross-validation experiments. Because of that, we did not implement a systematic search (e.g., GridSearchCV), having already identified this configuration as reliably optimal for our purposes.

    - Results: 

        + **On the not inflation adjusted data**: Our best model produced mse 1.4930 (R^2 value 0.0726) when trained without outliers.
     
        + **On the inflation adjusted data**: Our best model produced mse 25.5181 (R^2 value -0.0567) when trained without outliers.

## Future 

Modeling the targets we had with the parameters we had proved to be a difficult task. 

According to the EBM, the change in tech index is more related to its own historical data than other macroeconomic data. However, considering our best model (MLP with attention) was trained with no lag (i.e. it considers no historical data), this conclusion by EBM might be strictly specific to our preferred EBM models.  

The goal of our project was to see the impact of the price of resources on an industry, and stock index is the indicator of the state of the industry we chose. To keep studying from this perspective, we might seek to replace the indicator with something else. 

From another perspective, if we seek to study the behavior of the stock market: with lessons learnt from recent historical events, we might consider including surveys indicating people’s view on the economy into the parameters. 

The “Shallow dive” into deep learning was informative, we would like to do a “deeper dive” if possible. 


