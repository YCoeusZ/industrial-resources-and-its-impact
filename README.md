# industrial-resources-and-its-impact

## Folder organization  

<pre>
.
├──data 
    ├── collecting
    ├── processed
    ├── processing
        ├── DC01
        ├── DC02
        ├── PD01 
        ├── PD02 
        ├── PD03 
        ├── PD04
        └── PD05
    └── raw 
        ├── macro_daily
        ├── ppi_and_cpi
        └── stock
├── EDA 
    └── Yuan 
├── Erdos_milestone
    ├── Mar7_2025
    └── Mar21_2025
├── model_training
    ├── EBM
    ├── NN
    ├── spline
    └── SPLINE_YUAN
├── documents
└── proj_mod
</pre>

## Folder explanation

* ./data/collecting contains one file that collets PPI data with API. 

* ./data/processed contains all processed files generated by ipynb files.  

* ./data/processing organizes by steps of data processing.  

* ./data/raw organizes by categories the raw data files.  

* ./EDA organizes by partcipant the EDA related notebooks.  

* ./Erdos_milestone organizes by checkpoints the Erdos milestone notebooks.  

* ./model_training organizes by models the notebooks for model trainings.  

* ./documents contains documents submitted to Erdos data science boot camp: An excutive summary, a presentation ppt, and a video presentation. 

* ./proj_mod is the self created python module containing functions used accross files.  

## File naming explanation 
* All ipynb files are named with format: 

[*task code*]_[*further explanations*].ipynb 

See following the *task code* and *task* correspondence: 

| task code | task explanation | task dependency|
|-----------|-------------------|-------------|
|DC01|Data Cleaning 01: Identify stable tech company (i.e. a tech compnay, classified with NAICS, maintaining sp 500 index status during a continuous period that we care)|DC02|
|DC02|Data Cleaning 02: Identify the NAICS codes that we consider tech industry|NONE|
|DCO02|Data Collecting 02: Collecting PPI data according to the NAICS chosen with API|DC01|
|PD01|Processing Data 01: Created the *tech (stock) index*|DC01|
|PD02|Processing Data 02: Create proportional change of tech index|PD01|
|PD03|Processing Data 03: Processing daily macro data, this includes fed rates, resource prices, and alikes|NONE|
|PD04|Processing Data 04: Processing monthly macro data, this includes PPI data|DCO02|
|EDA|Doing EDA and playing around with the data|PD05 (returned to EDA after some alterations were made in PD05)|
|PD05|Processing Data 05: Create main datasets, this includes creating the no inflation adjusted dataset and creating the inlfation adjusted dataset|PD04,PD03,PD01,EDA (alterations are made after EDA)|
|MTEBM|Machine Training EBM: Training related files for Explanable Boosting Machine|PD05|
|MTMARS|Machine Training MARS: Training related files for Multivariate Adaptive Regression Splines|PD05|
|MTNN|Machine Training NN: Training related files for Neural Networks|PD05|
|MTSPLINE|Machine Training SPLINE: Training related files for Spline model|PD05| 

* ./data/data_process.py is designed to generate all needed processed data from raw (the only data collected through API is the PPI data, all other data are pre-requisite for the data processing step). 

    - In case the file does not operate correctly, please run .py files under ./data/ folder with order given in ./data/python_files_running_order.txt. 

## Contributors 
Kayode Oyedele, Rafael Almeida Fernandes, Thiago Brasileiro Feitosa, Wojciech Tralle, Xiangyi Tao, and Yuan Zhang. 

## Motivation 
It is a macroeconomics fact that the cost of production has an impact on the performance of an industry. We expect that the stock data of relevant industries will be a valid indicator of the performance of the industry. We, hence, look to use macroeconomic data and price data on industrial resources as parameters to train a supervised model on the target of stock data. In particular, we will focus solely on the technological industry in this project. We will care about the impact of contextual variables on the tech stock index: so instead of looking at the stock index itself, we will focus on the proportional change of the stock index relative to certain numbers of previous trading days; and, similarly, instead of the value of the parameters, we focus on the proportional change of those variables. However, certain variables are contextual on their own, for instance, the federal funds rate. 

As a clarification: the goal is NOT to predict the future of the stock market (e.g. predicting the index of tomorrow with data we have today). Instead, we are using the stock index as a representation of the state of the industry, and attempt to model the change in that state based on change on macroeconomic parameters (e.g. we may use proportional price change of copper today as one of the parameters for the target of proportional change of stock index of today). 

## Raw data collection 

* Daily data on the total S and P 500 stock index between 2009 and 2025. 
* Daily data of relative weight contribution of each S and P 500 companies. Notice that: Companies enter and exit S and P 500 throughout time depending on their performance. 
* SIC (Standard Industrial Classification codes), and stock tickers of companies.  
* The SIC to NAICS (North American Industry Classification System) crosswalk data. 
* Monthly data of PPI (Producer Price Index) classified by NAICS. These data are used as part of the parameters indicating the cost of production. 
* Daily data on raw industrial resources involved in the tech industry: gold, silver, copper, crude oil, palladium, platinum. These data are used as parameters influencing production costs. 
* Daily data on federal fund rates. These data are used as parameters. 
* Monthly data of CPI (Consumer Price Index). These data are used as indicators of general inflation. 

## Data Processing 

* Classifying the companies:
  
    - Challenge: Since the PPI data are classified by NAICS, but no raw data matching companies directly to NAICS exists.
  
      + Solution: combining raw data assigning companies to SIC codes and the crosswalk data on SIC to NAICS to produce a map from companies to NAICS codes.
      
    - Result: Produced, in accordance with NAICS, a list of companies considered as “tech industry”, and retrieved (monthly) PPI data according to the NAICS chosen.

* Producing Weighted Stock index of tech companies (i.e. the *tech (stock) index*):
  
    - Challenge: Companies leave the S and P 500 index periodically depending on their performance, and sometimes they also change names and/or stock tickers. To prevent “Jumps” happening in the tech stock index, we need to identify the “stable companies” that did not leave or enter the S and P 500 index in a period that we are interested in.
      
        + Solution: Created a dictionary mapping each company to all of its stock tickers used in history (as identifiers). Then identified the “stable companies” between 2014 and 2024 by ensuring there are no missing values on each trading date between the start and end.
          
    - Result: Created a list of “stable companies”, and combining the raw data on their weight contribution and total S and P index to receive the daily data on tech stock index:
```math
\text{Tech stock index}:= \text{Total index}\times(\sum_{\begin{matrix}\text{company }\in \\ \text{Stable tech companies}\end{matrix}}{\text{weight contribution of the company}})
```

* Creating proportional change daily data:
  
    - Challenge: sporadically missing data on certain days for price of raw resources.
      
        + Solution: Patched them up “linearly”.
          
    - Result: Generated daily proportional change data (recorded as percentage data by multiplying with 100) for each of the tech stock index and the raw resources with formula:
```math
\begin{matrix}
    \text{proportional change on day }t \\ 
    \text{relative to the }n^{th}\text{ prior day}
\end{matrix}:=\frac{\text{value on day }t}{\text{value on day }t-n}-1
```

* Create proportional change monthly data:
  
    - Result: Calculated the proportional change data in the context of monthly values and “expanded” them to daily values in the following way (e.g.): 
```math
\text{“Proportional change” on Jan/06/2018}:=\text{Proportional change on Jan/2018}
```

**Remark:** Due to the raw data collected, there is a chunk of missing data on tech stock index beginning Dec/31/2016, ending Jul/05/2017. This chunk is "too big to patch up" responsibly 

**Remark:** In EDA, it is noticed that the parameters are not highly correlated to the target (proportional change of tech stock index). And it appears that the proportional change of tech stock index tends to increase with the increase of proportional change of price of raw resources (this is opposed to our intuition). We **conjecture** that the counter intuitive trend can be explained by inflation, and thus the following step. 

* Create inflation adjusted data:
  
    - Challenge: Inflation is conventionally defined as the change rate of CPI (Customer Price index). This data is only in monthly format, but our data are all daily.
      
        + Solution: Instead of using proportional changes relative to 1st (trading) day prior, we will use the proportional changes relative to 20th (trading) day prior. As a remark, there are about 20 trading days in a calendar month. 

    - Result: Created inflation adjusted data relative to 20th (trading) day prior with formula:  
```math
\begin{matrix}
    \text{Inflation adjusted} \\ 
    \text{proportional change on day } t
\end{matrix}:=\frac{\text{value on day }t}{\text{value on day }t-20}\times \frac{\text{CPI previous month}}{\text{CPI current month}}-1
```

**Remark:** After another round of EDA, it can be seen that some of the correlations increased, but the counterintuitive behavior is intensified, this disproves our conjecture. 

## Datasets Created 

* **No inflation adjusted dataset**: For each row (one for each trading day), we have in columns:

    - **Target**: proportional change of tech stock index relative to previous trading day.
      
    - **Parameters**: proportional change of price of raw resources relative to previous trading day, proportional change of PPI’s relative to previous month, federal fund rate.
      
    - **Train and test split**: Test set start on date 2024-Jan-01 (Target has (Population) Variance: 1.6098). 

* **Inflation adjusted dataset**: For each row (one for each trading day), we have in columns: 

    - **Target**: inflation adjusted proportional change of tech stock index relative to 20th trading day prior. 

    - **Parameters**: inflation adjusted proportional change of price of raw resources relative to 20th trading day prior, inflation adjusted proportional change of PPI’s relative to previous month, federal fund rate.
 
    - **Train and test split**: Test set start on date 2024-Jan-01 (Target has (Population) Variance: 24.1491).
 
## EDA 

* **No Inflation adjustment dataset**:

The heatmap of correlations: \
    ![no_inf_corr_heat](https://github.com/user-attachments/assets/cb18b1c3-6a9c-4a24-b411-9e70d29a4afc)

In the EDA, the key observation is that there is very little correlation between the training target (seen on the first row in about heat map of correlations) and each of the single parameters.

An example of scatter plot between the daily stock index proportional change and the daily copper price proportional change: \
    ![no_inf_cop](https://github.com/user-attachments/assets/b04cb3eb-7061-4ff8-afd4-66bb88f0f8e0)

In fact, a counterintuitive behavior has been noticed: the stock index seem to (ever so slightly) increase when resource price is increasing (the graph above serves as an example, and the positive correlation indicates a positive sloped linear regression line). 
We **conjecture** that this is caused by inflation, which motivate the further data processing of creating inflation adjusted version of the data. 

And this conjeture is what cause us to, further, create the inflation adjusted dataset. 

* **Inflation adjusted dataset**:

The haatmap of correlations (inflation adjusted version, target is on the first row): \
    ![inf_corr_heat](https://github.com/user-attachments/assets/d4bd2618-348d-4602-a178-3b46daadce0e)

Scatter plot between the stock index proportional change and the copper price proportional change (both relative to the prior 20th day and inflation adjusted): \
    ![inf_cop](https://github.com/user-attachments/assets/796ac9c3-d227-4d77-83bd-302e22de1445)

The further data processing with inflation adjustment increase some of the correlations, and the counterintuitive behavior even became more pronounced in some cases. This disproves our conjecture. 

## Models, cross-validations, and results 

**The metric we will be using is mse (mean squared error). We will only provide graphs on the inflation adjusted dataset models in this README file.**

* **EBM (Explainable Boosting Machine)**: 

    - EBM team: Yuan, Kayode, Wojciech. 

    - Pipeline: 1,Index shifting to include more historical data -> 2, Exclude or include extreme training data -> 3, Standard scalar -> 4, EBM
 
    - Cross-validation: We decided to do cross-validation on changing shifting numbers (which decide how much historical data to include, the higher the shifting number, the more history is included), and if we were to include extreme training data. The cross-validation is done with time series 5-fold. 

    - Results:

       + **On the not inflation adjusted data**: It turns out that the higher the shifting, the better the EBM pipeline performs on validation sets without outliers, but worse when including outliers in the validation set. The impact of including or excluding outliers in the training data is small when the shifting number is small, but training with outliers produces better results when the shifting number is high. Our preferred final models produce: mse 1.5846 (R^2 value 0.0157) when trained with outliers and 10 shiftings, and mse 1.5694 (R^2 value 0.0251) when trained with outliers without any shiftings.
     
       + **On the inflation adjusted data**: It appears that training without outliers is preferred when the shifting number is low, but the difference is small when the shifting number is high. The error increases at first but then reduces while the shifting number increases. Including outliers in the validation set or not did not make much of a difference to the trends. Our preferred final model produces: mse 38.4721 (R^2 value -0.5931) when trained with extreme data and 3 shiftings.
     
         Scatter plot with red points being true values and blue points being predicted value (on the test set):\
         ![inf_adj_extreme_3_shifting](https://github.com/user-attachments/assets/b5671144-1aaf-43a2-b410-15b84999eb3b)

         Feature importance of preferred model provided by EBM:\
         ![feature_anal_inf_adj_extreme_shifted](https://github.com/user-attachments/assets/69e22c73-79ff-4bc3-af2d-4934b6449228)
         From top to bottom, the features are (all "days" mean "trading days" in the following list):
         
         + Tech index pro-change 20 days ago. 
         + Copper price pro-change 
         + Fed rate 20 days ago
         + Tech index pro-change 40 days ago
         + Gold price pro-change
         + Crude oil price pro-change 
         + PPI-5132 (software publisher) pro-change
         + PPI-517 (Telecommunications) pro-change 60 days ago
         + Crude oil price pro-change 40 days ago 
         + PPI-339 (Miscellaneous Durable Goods Manufacturing) pro-change 40 days ago 
         + PPI-332 (Fabricated metal product manufacturing) pro-change 40 days ago 
         + PPI-5132 (Software publisher) pro-change 60 days ago 
         + Platinum price pro-change 60 days ago

         An observation on the EBM model is that it depends on some of the parameters "consistently", we will use the following PDP generated with the interpret package on the "most important" parameter "Tech index pro-change 20 days ago" as an example:
         ![image](https://github.com/user-attachments/assets/7af24f95-89b5-492f-86fb-af4a4e5da925)

* **NN (Neural Network)**: 

    - NN team: Yuan, Kayode, Wojciech. 

    - Clarification: We only worked on the dataset with inflation adjustment. 

    - Cross-validation: We created two different models, one with a RNN (recurrent neural network) to include the impact of historical data, and a MLP with attention layer to include in the consideration of interactions. For each of these, we proceed with cross-validation by changing epoch numbers of each model. The attention layer model also includes cross-validation through including different lags to include lagged data from history. The RNN also lag length (determining how much past data to include).

    - Results:

      + **RNN**: Our preferred model produced mse 31.4635 (R^2 value -0.3029) with 190 epochs and 6 lag length.
     
        Scatter plot with red points being true values and blue points being predicted value (on the test set):\
        ![in_adj_rnn](https://github.com/user-attachments/assets/7ec4ae69-8df7-4958-bb9e-6ccb9c6107f6)

     
      + **MLP with Attention layer model**: Our preferred model produced mse 23.06 (R^2 value 0.0451) with no lag and 300 epochs.
     
        Scatter plot with red points being true values and blue points being predicted value (on the test set):\
        ![inf_adj_MLP_attention](https://github.com/user-attachments/assets/c24d31e4-4425-4506-a18c-b7efbd1ff1f1)
        
        **The MLP with attention layer model is our best model** 

     
* **SPLINE**: 

    - SPLINE team: Xiangyi, Thiago, Rafael.
 
    - Pipeline: 1, Exclude or include extreme training data -> 2, Standard scalar -> 3, SPLINE

    - Cross-validation: We used a five-fold time-series cross-validation. We explored different values for the spline transformer’s parameters (trying out several n_knots and degree settings by manually adjusting them). Through these manual checks, we found that n_knots=2 and degree=1 consistently yielded the lowest mse in our cross-validation experiments. Because of that, we did not implement a systematic search (e.g., GridSearchCV), having already identified this configuration as reliably optimal for our purposes.

    - Results: 

        + **On the not inflation adjusted data**: Our best model produced mse 1.4930 (R^2 value 0.0726) when trained without outliers.
     
        + **On the inflation adjusted data**: Our best model produced mse 25.5181 (R^2 value -0.0567) when trained without outliers.
     
          Scatter plot with red points being true values and blue points being predicted value (on the test set):\
          ![inf_adj_extreme_spline](https://github.com/user-attachments/assets/1ef5e09f-b659-4e9c-af50-75b4d46f646a)


## Future 

Modeling the targets we had with the parameters we had proved to be a difficult task. 

According to the EBM, the change in tech index is more related to its own historical data than other macroeconomic data. However, considering our best model (MLP with attention) was trained with no lag (i.e. it considers no historical data), this conclusion by EBM might be strictly specific to our preferred EBM models.  

The goal of our project was to see the impact of the price of resources on an industry, and stock index is the indicator of the state of the industry we chose. To keep studying from this perspective, we might seek to replace the indicator with something else. 

From another perspective, if we seek to study the behavior of the stock market: with lessons learnt from recent historical events, we might consider including surveys indicating people’s view on the economy into the parameters. 

The “Shallow dive” into deep learning was informative, we would like to do a “deeper dive” if possible. 


